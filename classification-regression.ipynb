{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4177, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3       4       5       6      7   8\n",
       "0  M  0.455  0.365  0.095  0.5140  0.2245  0.1010  0.150  15\n",
       "1  M  0.350  0.265  0.090  0.2255  0.0995  0.0485  0.070   7\n",
       "2  F  0.530  0.420  0.135  0.6770  0.2565  0.1415  0.210   9\n",
       "3  M  0.440  0.365  0.125  0.5160  0.2155  0.1140  0.155  10\n",
       "4  I  0.330  0.255  0.080  0.2050  0.0895  0.0395  0.055   7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and summarize the abalone dataset\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "# summarize shape\n",
    "print(dataframe.shape)\n",
    "# summarize first few lines\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'dataframe' is your pandas DataFrame\n",
    "# and you want to split it into input features (X) and output labels (y)\n",
    "# Use .iloc for positional indexing\n",
    "X = dataframe.iloc[:, 1:-1]\n",
    "y = dataframe.iloc[:, -1]\n",
    "\n",
    "# Convert to float if necessary\n",
    "X = X.astype('float')\n",
    "y = y.astype('float')\n",
    "\n",
    "# Get the number of features\n",
    "n_features = X.shape[1]\n",
    "\n",
    "print(f'Number of features: {n_features}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\ML-classification-regression\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m210\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m11\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">381</span> (1.49 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m381\u001b[0m (1.49 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">381</span> (1.49 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m381\u001b[0m (1.49 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "88/88 - 0s - 5ms/step - loss: 108.5178\n",
      "Epoch 2/150\n",
      "88/88 - 0s - 625us/step - loss: 71.9050\n",
      "Epoch 3/150\n",
      "88/88 - 0s - 591us/step - loss: 22.4573\n",
      "Epoch 4/150\n",
      "88/88 - 0s - 579us/step - loss: 7.7314\n",
      "Epoch 5/150\n",
      "88/88 - 0s - 579us/step - loss: 7.0976\n",
      "Epoch 6/150\n",
      "88/88 - 0s - 568us/step - loss: 6.8135\n",
      "Epoch 7/150\n",
      "88/88 - 0s - 580us/step - loss: 6.6240\n",
      "Epoch 8/150\n",
      "88/88 - 0s - 568us/step - loss: 6.4890\n",
      "Epoch 9/150\n",
      "88/88 - 0s - 579us/step - loss: 6.3941\n",
      "Epoch 10/150\n",
      "88/88 - 0s - 602us/step - loss: 6.3488\n",
      "Epoch 11/150\n",
      "88/88 - 0s - 579us/step - loss: 6.2845\n",
      "Epoch 12/150\n",
      "88/88 - 0s - 568us/step - loss: 6.2403\n",
      "Epoch 13/150\n",
      "88/88 - 0s - 568us/step - loss: 6.1770\n",
      "Epoch 14/150\n",
      "88/88 - 0s - 579us/step - loss: 6.1443\n",
      "Epoch 15/150\n",
      "88/88 - 0s - 579us/step - loss: 6.0811\n",
      "Epoch 16/150\n",
      "88/88 - 0s - 580us/step - loss: 6.0464\n",
      "Epoch 17/150\n",
      "88/88 - 0s - 625us/step - loss: 5.9939\n",
      "Epoch 18/150\n",
      "88/88 - 0s - 557us/step - loss: 5.9651\n",
      "Epoch 19/150\n",
      "88/88 - 0s - 557us/step - loss: 5.9056\n",
      "Epoch 20/150\n",
      "88/88 - 0s - 568us/step - loss: 5.8704\n",
      "Epoch 21/150\n",
      "88/88 - 0s - 568us/step - loss: 5.8014\n",
      "Epoch 22/150\n",
      "88/88 - 0s - 602us/step - loss: 5.7370\n",
      "Epoch 23/150\n",
      "88/88 - 0s - 693us/step - loss: 5.7014\n",
      "Epoch 24/150\n",
      "88/88 - 0s - 716us/step - loss: 5.6623\n",
      "Epoch 25/150\n",
      "88/88 - 0s - 591us/step - loss: 5.5964\n",
      "Epoch 26/150\n",
      "88/88 - 0s - 579us/step - loss: 5.5557\n",
      "Epoch 27/150\n",
      "88/88 - 0s - 568us/step - loss: 5.5241\n",
      "Epoch 28/150\n",
      "88/88 - 0s - 557us/step - loss: 5.4669\n",
      "Epoch 29/150\n",
      "88/88 - 0s - 568us/step - loss: 5.4741\n",
      "Epoch 30/150\n",
      "88/88 - 0s - 591us/step - loss: 5.3922\n",
      "Epoch 31/150\n",
      "88/88 - 0s - 602us/step - loss: 5.3635\n",
      "Epoch 32/150\n",
      "88/88 - 0s - 557us/step - loss: 5.3447\n",
      "Epoch 33/150\n",
      "88/88 - 0s - 557us/step - loss: 5.3369\n",
      "Epoch 34/150\n",
      "88/88 - 0s - 557us/step - loss: 5.2924\n",
      "Epoch 35/150\n",
      "88/88 - 0s - 591us/step - loss: 5.2554\n",
      "Epoch 36/150\n",
      "88/88 - 0s - 568us/step - loss: 5.2031\n",
      "Epoch 37/150\n",
      "88/88 - 0s - 580us/step - loss: 5.2051\n",
      "Epoch 38/150\n",
      "88/88 - 0s - 568us/step - loss: 5.1883\n",
      "Epoch 39/150\n",
      "88/88 - 0s - 568us/step - loss: 5.1421\n",
      "Epoch 40/150\n",
      "88/88 - 0s - 557us/step - loss: 5.1317\n",
      "Epoch 41/150\n",
      "88/88 - 0s - 591us/step - loss: 5.1214\n",
      "Epoch 42/150\n",
      "88/88 - 0s - 568us/step - loss: 5.1092\n",
      "Epoch 43/150\n",
      "88/88 - 0s - 579us/step - loss: 5.1131\n",
      "Epoch 44/150\n",
      "88/88 - 0s - 545us/step - loss: 5.0578\n",
      "Epoch 45/150\n",
      "88/88 - 0s - 557us/step - loss: 5.0555\n",
      "Epoch 46/150\n",
      "88/88 - 0s - 557us/step - loss: 5.0550\n",
      "Epoch 47/150\n",
      "88/88 - 0s - 568us/step - loss: 5.0252\n",
      "Epoch 48/150\n",
      "88/88 - 0s - 557us/step - loss: 5.0410\n",
      "Epoch 49/150\n",
      "88/88 - 0s - 557us/step - loss: 5.0356\n",
      "Epoch 50/150\n",
      "88/88 - 0s - 568us/step - loss: 4.9976\n",
      "Epoch 51/150\n",
      "88/88 - 0s - 568us/step - loss: 5.0067\n",
      "Epoch 52/150\n",
      "88/88 - 0s - 580us/step - loss: 4.9873\n",
      "Epoch 53/150\n",
      "88/88 - 0s - 557us/step - loss: 4.9921\n",
      "Epoch 54/150\n",
      "88/88 - 0s - 568us/step - loss: 4.9615\n",
      "Epoch 55/150\n",
      "88/88 - 0s - 568us/step - loss: 4.9709\n",
      "Epoch 56/150\n",
      "88/88 - 0s - 580us/step - loss: 4.9571\n",
      "Epoch 57/150\n",
      "88/88 - 0s - 545us/step - loss: 4.9395\n",
      "Epoch 58/150\n",
      "88/88 - 0s - 557us/step - loss: 4.9366\n",
      "Epoch 59/150\n",
      "88/88 - 0s - 545us/step - loss: 4.9482\n",
      "Epoch 60/150\n",
      "88/88 - 0s - 557us/step - loss: 4.9680\n",
      "Epoch 61/150\n",
      "88/88 - 0s - 579us/step - loss: 4.9773\n",
      "Epoch 62/150\n",
      "88/88 - 0s - 568us/step - loss: 4.9204\n",
      "Epoch 63/150\n",
      "88/88 - 0s - 545us/step - loss: 4.9047\n",
      "Epoch 64/150\n",
      "88/88 - 0s - 591us/step - loss: 4.9453\n",
      "Epoch 65/150\n",
      "88/88 - 0s - 568us/step - loss: 4.9079\n",
      "Epoch 66/150\n",
      "88/88 - 0s - 568us/step - loss: 4.9450\n",
      "Epoch 67/150\n",
      "88/88 - 0s - 580us/step - loss: 4.9088\n",
      "Epoch 68/150\n",
      "88/88 - 0s - 579us/step - loss: 4.8938\n",
      "Epoch 69/150\n",
      "88/88 - 0s - 579us/step - loss: 4.8775\n",
      "Epoch 70/150\n",
      "88/88 - 0s - 545us/step - loss: 4.9032\n",
      "Epoch 71/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8832\n",
      "Epoch 72/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8679\n",
      "Epoch 73/150\n",
      "88/88 - 0s - 545us/step - loss: 4.9468\n",
      "Epoch 74/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8753\n",
      "Epoch 75/150\n",
      "88/88 - 0s - 613us/step - loss: 4.8595\n",
      "Epoch 76/150\n",
      "88/88 - 0s - 580us/step - loss: 4.8737\n",
      "Epoch 77/150\n",
      "88/88 - 0s - 591us/step - loss: 4.8618\n",
      "Epoch 78/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8324\n",
      "Epoch 79/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8622\n",
      "Epoch 80/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8540\n",
      "Epoch 81/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8754\n",
      "Epoch 82/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8633\n",
      "Epoch 83/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8340\n",
      "Epoch 84/150\n",
      "88/88 - 0s - 591us/step - loss: 4.8424\n",
      "Epoch 85/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8357\n",
      "Epoch 86/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8275\n",
      "Epoch 87/150\n",
      "88/88 - 0s - 579us/step - loss: 4.8224\n",
      "Epoch 88/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8328\n",
      "Epoch 89/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8030\n",
      "Epoch 90/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8206\n",
      "Epoch 91/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8175\n",
      "Epoch 92/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7936\n",
      "Epoch 93/150\n",
      "88/88 - 0s - 614us/step - loss: 4.8244\n",
      "Epoch 94/150\n",
      "88/88 - 0s - 830us/step - loss: 4.7977\n",
      "Epoch 95/150\n",
      "88/88 - 0s - 636us/step - loss: 4.7872\n",
      "Epoch 96/150\n",
      "88/88 - 0s - 614us/step - loss: 4.7784\n",
      "Epoch 97/150\n",
      "88/88 - 0s - 568us/step - loss: 4.7998\n",
      "Epoch 98/150\n",
      "88/88 - 0s - 568us/step - loss: 4.7678\n",
      "Epoch 99/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7722\n",
      "Epoch 100/150\n",
      "88/88 - 0s - 614us/step - loss: 4.7733\n",
      "Epoch 101/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7969\n",
      "Epoch 102/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7536\n",
      "Epoch 103/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7835\n",
      "Epoch 104/150\n",
      "88/88 - 0s - 568us/step - loss: 4.7703\n",
      "Epoch 105/150\n",
      "88/88 - 0s - 579us/step - loss: 4.7582\n",
      "Epoch 106/150\n",
      "88/88 - 0s - 591us/step - loss: 4.7924\n",
      "Epoch 107/150\n",
      "88/88 - 0s - 614us/step - loss: 4.7556\n",
      "Epoch 108/150\n",
      "88/88 - 0s - 580us/step - loss: 4.7451\n",
      "Epoch 109/150\n",
      "88/88 - 0s - 579us/step - loss: 4.7741\n",
      "Epoch 110/150\n",
      "88/88 - 0s - 534us/step - loss: 4.7387\n",
      "Epoch 111/150\n",
      "88/88 - 0s - 546us/step - loss: 4.7568\n",
      "Epoch 112/150\n",
      "88/88 - 0s - 636us/step - loss: 4.7401\n",
      "Epoch 113/150\n",
      "88/88 - 0s - 625us/step - loss: 4.7485\n",
      "Epoch 114/150\n",
      "88/88 - 0s - 580us/step - loss: 4.7275\n",
      "Epoch 115/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7237\n",
      "Epoch 116/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7258\n",
      "Epoch 117/150\n",
      "88/88 - 0s - 579us/step - loss: 4.7615\n",
      "Epoch 118/150\n",
      "88/88 - 0s - 591us/step - loss: 4.7250\n",
      "Epoch 119/150\n",
      "88/88 - 0s - 613us/step - loss: 4.7064\n",
      "Epoch 120/150\n",
      "88/88 - 0s - 579us/step - loss: 4.7340\n",
      "Epoch 121/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7078\n",
      "Epoch 122/150\n",
      "88/88 - 0s - 546us/step - loss: 4.7273\n",
      "Epoch 123/150\n",
      "88/88 - 0s - 602us/step - loss: 4.7143\n",
      "Epoch 124/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7088\n",
      "Epoch 125/150\n",
      "88/88 - 0s - 534us/step - loss: 4.7067\n",
      "Epoch 126/150\n",
      "88/88 - 0s - 545us/step - loss: 4.6967\n",
      "Epoch 127/150\n",
      "88/88 - 0s - 534us/step - loss: 4.7133\n",
      "Epoch 128/150\n",
      "88/88 - 0s - 545us/step - loss: 4.6914\n",
      "Epoch 129/150\n",
      "88/88 - 0s - 614us/step - loss: 4.6979\n",
      "Epoch 130/150\n",
      "88/88 - 0s - 557us/step - loss: 4.6818\n",
      "Epoch 131/150\n",
      "88/88 - 0s - 557us/step - loss: 4.6858\n",
      "Epoch 132/150\n",
      "88/88 - 0s - 568us/step - loss: 4.7297\n",
      "Epoch 133/150\n",
      "88/88 - 0s - 602us/step - loss: 4.6745\n",
      "Epoch 134/150\n",
      "88/88 - 0s - 568us/step - loss: 4.6902\n",
      "Epoch 135/150\n",
      "88/88 - 0s - 580us/step - loss: 4.7162\n",
      "Epoch 136/150\n",
      "88/88 - 0s - 579us/step - loss: 4.6911\n",
      "Epoch 137/150\n",
      "88/88 - 0s - 557us/step - loss: 4.6736\n",
      "Epoch 138/150\n",
      "88/88 - 0s - 568us/step - loss: 4.6874\n",
      "Epoch 139/150\n",
      "88/88 - 0s - 591us/step - loss: 4.6862\n",
      "Epoch 140/150\n",
      "88/88 - 0s - 579us/step - loss: 4.6530\n",
      "Epoch 141/150\n",
      "88/88 - 0s - 568us/step - loss: 4.6953\n",
      "Epoch 142/150\n",
      "88/88 - 0s - 545us/step - loss: 4.6834\n",
      "Epoch 143/150\n",
      "88/88 - 0s - 602us/step - loss: 4.6508\n",
      "Epoch 144/150\n",
      "88/88 - 0s - 727us/step - loss: 4.6470\n",
      "Epoch 145/150\n",
      "88/88 - 0s - 659us/step - loss: 4.6461\n",
      "Epoch 146/150\n",
      "88/88 - 0s - 636us/step - loss: 4.6529\n",
      "Epoch 147/150\n",
      "88/88 - 0s - 591us/step - loss: 4.6668\n",
      "Epoch 148/150\n",
      "88/88 - 0s - 568us/step - loss: 4.6650\n",
      "Epoch 149/150\n",
      "88/88 - 0s - 545us/step - loss: 4.6541\n",
      "Epoch 150/150\n",
      "88/88 - 0s - 557us/step - loss: 4.6702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x142bff66ab0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step\n",
      "MAE: 1.535\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "error = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tying this all together, the complete example of an MLP neural network for the abalone dataset framed as a regression problem is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\ML-classification-regression\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 0s - 5ms/step - loss: 101.9918\n",
      "Epoch 2/150\n",
      "88/88 - 0s - 602us/step - loss: 70.2908\n",
      "Epoch 3/150\n",
      "88/88 - 0s - 557us/step - loss: 30.3110\n",
      "Epoch 4/150\n",
      "88/88 - 0s - 568us/step - loss: 10.4231\n",
      "Epoch 5/150\n",
      "88/88 - 0s - 580us/step - loss: 8.5522\n",
      "Epoch 6/150\n",
      "88/88 - 0s - 807us/step - loss: 8.3069\n",
      "Epoch 7/150\n",
      "88/88 - 0s - 591us/step - loss: 8.0234\n",
      "Epoch 8/150\n",
      "88/88 - 0s - 591us/step - loss: 7.8015\n",
      "Epoch 9/150\n",
      "88/88 - 0s - 579us/step - loss: 7.5893\n",
      "Epoch 10/150\n",
      "88/88 - 0s - 580us/step - loss: 7.3975\n",
      "Epoch 11/150\n",
      "88/88 - 0s - 545us/step - loss: 7.2027\n",
      "Epoch 12/150\n",
      "88/88 - 0s - 545us/step - loss: 7.0207\n",
      "Epoch 13/150\n",
      "88/88 - 0s - 557us/step - loss: 6.8553\n",
      "Epoch 14/150\n",
      "88/88 - 0s - 568us/step - loss: 6.7136\n",
      "Epoch 15/150\n",
      "88/88 - 0s - 557us/step - loss: 6.5860\n",
      "Epoch 16/150\n",
      "88/88 - 0s - 602us/step - loss: 6.4786\n",
      "Epoch 17/150\n",
      "88/88 - 0s - 557us/step - loss: 6.3711\n",
      "Epoch 18/150\n",
      "88/88 - 0s - 545us/step - loss: 6.2646\n",
      "Epoch 19/150\n",
      "88/88 - 0s - 557us/step - loss: 6.1564\n",
      "Epoch 20/150\n",
      "88/88 - 0s - 557us/step - loss: 6.0245\n",
      "Epoch 21/150\n",
      "88/88 - 0s - 557us/step - loss: 5.8819\n",
      "Epoch 22/150\n",
      "88/88 - 0s - 545us/step - loss: 5.7698\n",
      "Epoch 23/150\n",
      "88/88 - 0s - 557us/step - loss: 5.6640\n",
      "Epoch 24/150\n",
      "88/88 - 0s - 602us/step - loss: 5.5545\n",
      "Epoch 25/150\n",
      "88/88 - 0s - 545us/step - loss: 5.4415\n",
      "Epoch 26/150\n",
      "88/88 - 0s - 545us/step - loss: 5.3549\n",
      "Epoch 27/150\n",
      "88/88 - 0s - 545us/step - loss: 5.2900\n",
      "Epoch 28/150\n",
      "88/88 - 0s - 545us/step - loss: 5.2192\n",
      "Epoch 29/150\n",
      "88/88 - 0s - 568us/step - loss: 5.1561\n",
      "Epoch 30/150\n",
      "88/88 - 0s - 557us/step - loss: 5.1314\n",
      "Epoch 31/150\n",
      "88/88 - 0s - 545us/step - loss: 5.0820\n",
      "Epoch 32/150\n",
      "88/88 - 0s - 557us/step - loss: 5.0742\n",
      "Epoch 33/150\n",
      "88/88 - 0s - 568us/step - loss: 5.0475\n",
      "Epoch 34/150\n",
      "88/88 - 0s - 568us/step - loss: 4.9985\n",
      "Epoch 35/150\n",
      "88/88 - 0s - 580us/step - loss: 4.9828\n",
      "Epoch 36/150\n",
      "88/88 - 0s - 591us/step - loss: 4.9615\n",
      "Epoch 37/150\n",
      "88/88 - 0s - 580us/step - loss: 4.9478\n",
      "Epoch 38/150\n",
      "88/88 - 0s - 579us/step - loss: 4.9518\n",
      "Epoch 39/150\n",
      "88/88 - 0s - 568us/step - loss: 4.9371\n",
      "Epoch 40/150\n",
      "88/88 - 0s - 580us/step - loss: 4.9129\n",
      "Epoch 41/150\n",
      "88/88 - 0s - 568us/step - loss: 4.9303\n",
      "Epoch 42/150\n",
      "88/88 - 0s - 580us/step - loss: 4.9062\n",
      "Epoch 43/150\n",
      "88/88 - 0s - 579us/step - loss: 4.9042\n",
      "Epoch 44/150\n",
      "88/88 - 0s - 568us/step - loss: 4.9442\n",
      "Epoch 45/150\n",
      "88/88 - 0s - 580us/step - loss: 4.9190\n",
      "Epoch 46/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8950\n",
      "Epoch 47/150\n",
      "88/88 - 0s - 625us/step - loss: 4.8818\n",
      "Epoch 48/150\n",
      "88/88 - 0s - 636us/step - loss: 4.8863\n",
      "Epoch 49/150\n",
      "88/88 - 0s - 636us/step - loss: 4.8898\n",
      "Epoch 50/150\n",
      "88/88 - 0s - 636us/step - loss: 4.8712\n",
      "Epoch 51/150\n",
      "88/88 - 0s - 614us/step - loss: 4.8789\n",
      "Epoch 52/150\n",
      "88/88 - 0s - 591us/step - loss: 4.8668\n",
      "Epoch 53/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8853\n",
      "Epoch 54/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8634\n",
      "Epoch 55/150\n",
      "88/88 - 0s - 591us/step - loss: 4.8786\n",
      "Epoch 56/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8783\n",
      "Epoch 57/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8686\n",
      "Epoch 58/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8851\n",
      "Epoch 59/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8699\n",
      "Epoch 60/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8560\n",
      "Epoch 61/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8667\n",
      "Epoch 62/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8646\n",
      "Epoch 63/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8592\n",
      "Epoch 64/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8535\n",
      "Epoch 65/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8923\n",
      "Epoch 66/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8503\n",
      "Epoch 67/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8549\n",
      "Epoch 68/150\n",
      "88/88 - 0s - 591us/step - loss: 4.8641\n",
      "Epoch 69/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8821\n",
      "Epoch 70/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8510\n",
      "Epoch 71/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8449\n",
      "Epoch 72/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8471\n",
      "Epoch 73/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8504\n",
      "Epoch 74/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8467\n",
      "Epoch 75/150\n",
      "88/88 - 0s - 579us/step - loss: 4.8471\n",
      "Epoch 76/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8641\n",
      "Epoch 77/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8470\n",
      "Epoch 78/150\n",
      "88/88 - 0s - 579us/step - loss: 4.8373\n",
      "Epoch 79/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8395\n",
      "Epoch 80/150\n",
      "88/88 - 0s - 580us/step - loss: 4.8548\n",
      "Epoch 81/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8303\n",
      "Epoch 82/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8272\n",
      "Epoch 83/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8314\n",
      "Epoch 84/150\n",
      "88/88 - 0s - 614us/step - loss: 4.8211\n",
      "Epoch 85/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8065\n",
      "Epoch 86/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8036\n",
      "Epoch 87/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8262\n",
      "Epoch 88/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7970\n",
      "Epoch 89/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8418\n",
      "Epoch 90/150\n",
      "88/88 - 0s - 534us/step - loss: 4.8171\n",
      "Epoch 91/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8249\n",
      "Epoch 92/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8068\n",
      "Epoch 93/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7974\n",
      "Epoch 94/150\n",
      "88/88 - 0s - 568us/step - loss: 4.8053\n",
      "Epoch 95/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8089\n",
      "Epoch 96/150\n",
      "88/88 - 0s - 557us/step - loss: 4.8101\n",
      "Epoch 97/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7891\n",
      "Epoch 98/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7936\n",
      "Epoch 99/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7849\n",
      "Epoch 100/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8331\n",
      "Epoch 101/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7836\n",
      "Epoch 102/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7885\n",
      "Epoch 103/150\n",
      "88/88 - 0s - 534us/step - loss: 4.7747\n",
      "Epoch 104/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7856\n",
      "Epoch 105/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7662\n",
      "Epoch 106/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7710\n",
      "Epoch 107/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7727\n",
      "Epoch 108/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7710\n",
      "Epoch 109/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7692\n",
      "Epoch 110/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7698\n",
      "Epoch 111/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7771\n",
      "Epoch 112/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7638\n",
      "Epoch 113/150\n",
      "88/88 - 0s - 534us/step - loss: 4.7920\n",
      "Epoch 114/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7775\n",
      "Epoch 115/150\n",
      "88/88 - 0s - 568us/step - loss: 4.7587\n",
      "Epoch 116/150\n",
      "88/88 - 0s - 534us/step - loss: 4.7616\n",
      "Epoch 117/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7863\n",
      "Epoch 118/150\n",
      "88/88 - 0s - 545us/step - loss: 4.8188\n",
      "Epoch 119/150\n",
      "88/88 - 0s - 534us/step - loss: 4.7646\n",
      "Epoch 120/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7683\n",
      "Epoch 121/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7597\n",
      "Epoch 122/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7407\n",
      "Epoch 123/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7617\n",
      "Epoch 124/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7561\n",
      "Epoch 125/150\n",
      "88/88 - 0s - 534us/step - loss: 4.7504\n",
      "Epoch 126/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7656\n",
      "Epoch 127/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7725\n",
      "Epoch 128/150\n",
      "88/88 - 0s - 568us/step - loss: 4.7480\n",
      "Epoch 129/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7843\n",
      "Epoch 130/150\n",
      "88/88 - 0s - 568us/step - loss: 4.7575\n",
      "Epoch 131/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7665\n",
      "Epoch 132/150\n",
      "88/88 - 0s - 534us/step - loss: 4.7436\n",
      "Epoch 133/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7522\n",
      "Epoch 134/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7378\n",
      "Epoch 135/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7517\n",
      "Epoch 136/150\n",
      "88/88 - 0s - 568us/step - loss: 4.7381\n",
      "Epoch 137/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7546\n",
      "Epoch 138/150\n",
      "88/88 - 0s - 557us/step - loss: 4.7413\n",
      "Epoch 139/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7378\n",
      "Epoch 140/150\n",
      "88/88 - 0s - 795us/step - loss: 4.7671\n",
      "Epoch 141/150\n",
      "88/88 - 0s - 591us/step - loss: 4.7614\n",
      "Epoch 142/150\n",
      "88/88 - 0s - 579us/step - loss: 4.7279\n",
      "Epoch 143/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7320\n",
      "Epoch 144/150\n",
      "88/88 - 0s - 568us/step - loss: 4.7602\n",
      "Epoch 145/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7378\n",
      "Epoch 146/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7565\n",
      "Epoch 147/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7290\n",
      "Epoch 148/150\n",
      "88/88 - 0s - 545us/step - loss: 4.7214\n",
      "Epoch 149/150\n",
      "88/88 - 0s - 568us/step - loss: 4.7262\n",
      "Epoch 150/150\n",
      "88/88 - 0s - 602us/step - loss: 4.7316\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step\n",
      "MAE: 1.568\n"
     ]
    }
   ],
   "source": [
    "# regression mlp model for the abalone dataset\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (y) variables\n",
    "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
    "X, y = X.astype('float'), y.astype('float')\n",
    "n_features = X.shape[1]\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# compile the keras model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "error = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification mlp model for the abalone dataset\n",
    "from numpy import unique\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Assuming 'dataframe' is your pandas DataFrame\n",
    "# and you want to split it into input features (X) and output labels (y)\n",
    "# Use .iloc for positional indexing\n",
    "X = dataframe.iloc[:, 1:-1]\n",
    "# Convert to float if necessary\n",
    "X = X.astype('float')\n",
    "# Get the number of features\n",
    "n_features = X.shape[1]\n",
    "\n",
    "print(f'Number of features: {n_features}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14  6  8 ...  8  9 11]\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "# encode strings to integer\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "print(y)\n",
    "n_class = len(unique(y))\n",
    "print(n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\ML-classification-regression\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(n_class, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">308</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m210\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)             │           \u001b[38;5;34m308\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">678</span> (2.65 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m678\u001b[0m (2.65 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">678</span> (2.65 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m678\u001b[0m (2.65 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "105/105 - 0s - 4ms/step - loss: 3.0357\n",
      "Epoch 2/500\n",
      "105/105 - 0s - 590us/step - loss: 2.5997\n",
      "Epoch 3/500\n",
      "105/105 - 0s - 562us/step - loss: 2.5052\n",
      "Epoch 4/500\n",
      "105/105 - 0s - 543us/step - loss: 2.4551\n",
      "Epoch 5/500\n",
      "105/105 - 0s - 533us/step - loss: 2.3994\n",
      "Epoch 6/500\n",
      "105/105 - 0s - 533us/step - loss: 2.3047\n",
      "Epoch 7/500\n",
      "105/105 - 0s - 533us/step - loss: 2.2308\n",
      "Epoch 8/500\n",
      "105/105 - 0s - 533us/step - loss: 2.1933\n",
      "Epoch 9/500\n",
      "105/105 - 0s - 533us/step - loss: 2.1724\n",
      "Epoch 10/500\n",
      "105/105 - 0s - 552us/step - loss: 2.1589\n",
      "Epoch 11/500\n",
      "105/105 - 0s - 581us/step - loss: 2.1484\n",
      "Epoch 12/500\n",
      "105/105 - 0s - 571us/step - loss: 2.1398\n",
      "Epoch 13/500\n",
      "105/105 - 0s - 552us/step - loss: 2.1323\n",
      "Epoch 14/500\n",
      "105/105 - 0s - 543us/step - loss: 2.1261\n",
      "Epoch 15/500\n",
      "105/105 - 0s - 534us/step - loss: 2.1186\n",
      "Epoch 16/500\n",
      "105/105 - 0s - 533us/step - loss: 2.1127\n",
      "Epoch 17/500\n",
      "105/105 - 0s - 533us/step - loss: 2.1078\n",
      "Epoch 18/500\n",
      "105/105 - 0s - 561us/step - loss: 2.1019\n",
      "Epoch 19/500\n",
      "105/105 - 0s - 571us/step - loss: 2.0958\n",
      "Epoch 20/500\n",
      "105/105 - 0s - 552us/step - loss: 2.0890\n",
      "Epoch 21/500\n",
      "105/105 - 0s - 552us/step - loss: 2.0832\n",
      "Epoch 22/500\n",
      "105/105 - 0s - 543us/step - loss: 2.0785\n",
      "Epoch 23/500\n",
      "105/105 - 0s - 562us/step - loss: 2.0723\n",
      "Epoch 24/500\n",
      "105/105 - 0s - 533us/step - loss: 2.0677\n",
      "Epoch 25/500\n",
      "105/105 - 0s - 562us/step - loss: 2.0607\n",
      "Epoch 26/500\n",
      "105/105 - 0s - 571us/step - loss: 2.0562\n",
      "Epoch 27/500\n",
      "105/105 - 0s - 552us/step - loss: 2.0506\n",
      "Epoch 28/500\n",
      "105/105 - 0s - 552us/step - loss: 2.0467\n",
      "Epoch 29/500\n",
      "105/105 - 0s - 533us/step - loss: 2.0400\n",
      "Epoch 30/500\n",
      "105/105 - 0s - 543us/step - loss: 2.0353\n",
      "Epoch 31/500\n",
      "105/105 - 0s - 543us/step - loss: 2.0311\n",
      "Epoch 32/500\n",
      "105/105 - 0s - 533us/step - loss: 2.0262\n",
      "Epoch 33/500\n",
      "105/105 - 0s - 543us/step - loss: 2.0229\n",
      "Epoch 34/500\n",
      "105/105 - 0s - 543us/step - loss: 2.0170\n",
      "Epoch 35/500\n",
      "105/105 - 0s - 543us/step - loss: 2.0148\n",
      "Epoch 36/500\n",
      "105/105 - 0s - 533us/step - loss: 2.0083\n",
      "Epoch 37/500\n",
      "105/105 - 0s - 552us/step - loss: 2.0056\n",
      "Epoch 38/500\n",
      "105/105 - 0s - 543us/step - loss: 2.0007\n",
      "Epoch 39/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9974\n",
      "Epoch 40/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9937\n",
      "Epoch 41/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9927\n",
      "Epoch 42/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9901\n",
      "Epoch 43/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9861\n",
      "Epoch 44/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9828\n",
      "Epoch 45/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9801\n",
      "Epoch 46/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9788\n",
      "Epoch 47/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9760\n",
      "Epoch 48/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9728\n",
      "Epoch 49/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9727\n",
      "Epoch 50/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9694\n",
      "Epoch 51/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9678\n",
      "Epoch 52/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9662\n",
      "Epoch 53/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9636\n",
      "Epoch 54/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9619\n",
      "Epoch 55/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9613\n",
      "Epoch 56/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9603\n",
      "Epoch 57/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9601\n",
      "Epoch 58/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9577\n",
      "Epoch 59/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9559\n",
      "Epoch 60/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9573\n",
      "Epoch 61/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9549\n",
      "Epoch 62/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9529\n",
      "Epoch 63/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9562\n",
      "Epoch 64/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9504\n",
      "Epoch 65/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9520\n",
      "Epoch 66/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9510\n",
      "Epoch 67/500\n",
      "105/105 - 0s - 619us/step - loss: 1.9509\n",
      "Epoch 68/500\n",
      "105/105 - 0s - 638us/step - loss: 1.9481\n",
      "Epoch 69/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9494\n",
      "Epoch 70/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9458\n",
      "Epoch 71/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9463\n",
      "Epoch 72/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9479\n",
      "Epoch 73/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9462\n",
      "Epoch 74/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9450\n",
      "Epoch 75/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9448\n",
      "Epoch 76/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9460\n",
      "Epoch 77/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9428\n",
      "Epoch 78/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9435\n",
      "Epoch 79/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9416\n",
      "Epoch 80/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9405\n",
      "Epoch 81/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9426\n",
      "Epoch 82/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9406\n",
      "Epoch 83/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9398\n",
      "Epoch 84/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9411\n",
      "Epoch 85/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9390\n",
      "Epoch 86/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9400\n",
      "Epoch 87/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9394\n",
      "Epoch 88/500\n",
      "105/105 - 0s - 571us/step - loss: 1.9357\n",
      "Epoch 89/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9375\n",
      "Epoch 90/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9371\n",
      "Epoch 91/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9368\n",
      "Epoch 92/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9373\n",
      "Epoch 93/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9363\n",
      "Epoch 94/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9349\n",
      "Epoch 95/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9354\n",
      "Epoch 96/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9346\n",
      "Epoch 97/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9358\n",
      "Epoch 98/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9356\n",
      "Epoch 99/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9357\n",
      "Epoch 100/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9368\n",
      "Epoch 101/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9336\n",
      "Epoch 102/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9338\n",
      "Epoch 103/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9332\n",
      "Epoch 104/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9334\n",
      "Epoch 105/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9330\n",
      "Epoch 106/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9337\n",
      "Epoch 107/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9321\n",
      "Epoch 108/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9323\n",
      "Epoch 109/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9323\n",
      "Epoch 110/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9337\n",
      "Epoch 111/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9324\n",
      "Epoch 112/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9293\n",
      "Epoch 113/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9320\n",
      "Epoch 114/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9304\n",
      "Epoch 115/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9319\n",
      "Epoch 116/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9322\n",
      "Epoch 117/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9302\n",
      "Epoch 118/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9297\n",
      "Epoch 119/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9282\n",
      "Epoch 120/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9314\n",
      "Epoch 121/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9312\n",
      "Epoch 122/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9281\n",
      "Epoch 123/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9294\n",
      "Epoch 124/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9287\n",
      "Epoch 125/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9298\n",
      "Epoch 126/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9278\n",
      "Epoch 127/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9289\n",
      "Epoch 128/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9286\n",
      "Epoch 129/500\n",
      "105/105 - 0s - 553us/step - loss: 1.9281\n",
      "Epoch 130/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9272\n",
      "Epoch 131/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9281\n",
      "Epoch 132/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9251\n",
      "Epoch 133/500\n",
      "105/105 - 0s - 571us/step - loss: 1.9281\n",
      "Epoch 134/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9275\n",
      "Epoch 135/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9281\n",
      "Epoch 136/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9270\n",
      "Epoch 137/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9281\n",
      "Epoch 138/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9260\n",
      "Epoch 139/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9265\n",
      "Epoch 140/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9256\n",
      "Epoch 141/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9254\n",
      "Epoch 142/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9264\n",
      "Epoch 143/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9258\n",
      "Epoch 144/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9268\n",
      "Epoch 145/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9253\n",
      "Epoch 146/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9279\n",
      "Epoch 147/500\n",
      "105/105 - 0s - 571us/step - loss: 1.9258\n",
      "Epoch 148/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9253\n",
      "Epoch 149/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9245\n",
      "Epoch 150/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9245\n",
      "Epoch 151/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9240\n",
      "Epoch 152/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9234\n",
      "Epoch 153/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9255\n",
      "Epoch 154/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9231\n",
      "Epoch 155/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9240\n",
      "Epoch 156/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9257\n",
      "Epoch 157/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9215\n",
      "Epoch 158/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9251\n",
      "Epoch 159/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9237\n",
      "Epoch 160/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9220\n",
      "Epoch 161/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9224\n",
      "Epoch 162/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9235\n",
      "Epoch 163/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9240\n",
      "Epoch 164/500\n",
      "105/105 - 0s - 542us/step - loss: 1.9220\n",
      "Epoch 165/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9224\n",
      "Epoch 166/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9233\n",
      "Epoch 167/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9238\n",
      "Epoch 168/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9202\n",
      "Epoch 169/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9219\n",
      "Epoch 170/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9228\n",
      "Epoch 171/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9229\n",
      "Epoch 172/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9204\n",
      "Epoch 173/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9225\n",
      "Epoch 174/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9208\n",
      "Epoch 175/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9208\n",
      "Epoch 176/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9207\n",
      "Epoch 177/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9205\n",
      "Epoch 178/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9210\n",
      "Epoch 179/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9200\n",
      "Epoch 180/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9211\n",
      "Epoch 181/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9204\n",
      "Epoch 182/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9202\n",
      "Epoch 183/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9201\n",
      "Epoch 184/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9202\n",
      "Epoch 185/500\n",
      "105/105 - 0s - 571us/step - loss: 1.9186\n",
      "Epoch 186/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9209\n",
      "Epoch 187/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9205\n",
      "Epoch 188/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9194\n",
      "Epoch 189/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9206\n",
      "Epoch 190/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9205\n",
      "Epoch 191/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9201\n",
      "Epoch 192/500\n",
      "105/105 - 0s - 542us/step - loss: 1.9191\n",
      "Epoch 193/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9191\n",
      "Epoch 194/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9194\n",
      "Epoch 195/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9192\n",
      "Epoch 196/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9192\n",
      "Epoch 197/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9164\n",
      "Epoch 198/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9187\n",
      "Epoch 199/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9192\n",
      "Epoch 200/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9181\n",
      "Epoch 201/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9211\n",
      "Epoch 202/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9193\n",
      "Epoch 203/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9197\n",
      "Epoch 204/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9181\n",
      "Epoch 205/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9178\n",
      "Epoch 206/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9188\n",
      "Epoch 207/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9178\n",
      "Epoch 208/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9168\n",
      "Epoch 209/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9173\n",
      "Epoch 210/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9196\n",
      "Epoch 211/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9173\n",
      "Epoch 212/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9166\n",
      "Epoch 213/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9163\n",
      "Epoch 214/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9173\n",
      "Epoch 215/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9160\n",
      "Epoch 216/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9172\n",
      "Epoch 217/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9159\n",
      "Epoch 218/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9164\n",
      "Epoch 219/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9165\n",
      "Epoch 220/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9167\n",
      "Epoch 221/500\n",
      "105/105 - 0s - 534us/step - loss: 1.9164\n",
      "Epoch 222/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9162\n",
      "Epoch 223/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9165\n",
      "Epoch 224/500\n",
      "105/105 - 0s - 762us/step - loss: 1.9164\n",
      "Epoch 225/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9172\n",
      "Epoch 226/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9149\n",
      "Epoch 227/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9159\n",
      "Epoch 228/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9152\n",
      "Epoch 229/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9157\n",
      "Epoch 230/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9142\n",
      "Epoch 231/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9144\n",
      "Epoch 232/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9151\n",
      "Epoch 233/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9146\n",
      "Epoch 234/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9155\n",
      "Epoch 235/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9167\n",
      "Epoch 236/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9160\n",
      "Epoch 237/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9141\n",
      "Epoch 238/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9145\n",
      "Epoch 239/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9139\n",
      "Epoch 240/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9160\n",
      "Epoch 241/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9146\n",
      "Epoch 242/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9151\n",
      "Epoch 243/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9125\n",
      "Epoch 244/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9144\n",
      "Epoch 245/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9144\n",
      "Epoch 246/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9155\n",
      "Epoch 247/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9145\n",
      "Epoch 248/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9139\n",
      "Epoch 249/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9146\n",
      "Epoch 250/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9144\n",
      "Epoch 251/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9133\n",
      "Epoch 252/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9128\n",
      "Epoch 253/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9169\n",
      "Epoch 254/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9129\n",
      "Epoch 255/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9141\n",
      "Epoch 256/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9123\n",
      "Epoch 257/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9148\n",
      "Epoch 258/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9135\n",
      "Epoch 259/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9136\n",
      "Epoch 260/500\n",
      "105/105 - 0s - 534us/step - loss: 1.9135\n",
      "Epoch 261/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9117\n",
      "Epoch 262/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9118\n",
      "Epoch 263/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9132\n",
      "Epoch 264/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9151\n",
      "Epoch 265/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9112\n",
      "Epoch 266/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9121\n",
      "Epoch 267/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9129\n",
      "Epoch 268/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9115\n",
      "Epoch 269/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9116\n",
      "Epoch 270/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9112\n",
      "Epoch 271/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9124\n",
      "Epoch 272/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9126\n",
      "Epoch 273/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9115\n",
      "Epoch 274/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9143\n",
      "Epoch 275/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9121\n",
      "Epoch 276/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9116\n",
      "Epoch 277/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9114\n",
      "Epoch 278/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9125\n",
      "Epoch 279/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9130\n",
      "Epoch 280/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9100\n",
      "Epoch 281/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9120\n",
      "Epoch 282/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9115\n",
      "Epoch 283/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9111\n",
      "Epoch 284/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9131\n",
      "Epoch 285/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9108\n",
      "Epoch 286/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9093\n",
      "Epoch 287/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9123\n",
      "Epoch 288/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9133\n",
      "Epoch 289/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9112\n",
      "Epoch 290/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9098\n",
      "Epoch 291/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9102\n",
      "Epoch 292/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9109\n",
      "Epoch 293/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9105\n",
      "Epoch 294/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9112\n",
      "Epoch 295/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9104\n",
      "Epoch 296/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9096\n",
      "Epoch 297/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9114\n",
      "Epoch 298/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9108\n",
      "Epoch 299/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9122\n",
      "Epoch 300/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9126\n",
      "Epoch 301/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9102\n",
      "Epoch 302/500\n",
      "105/105 - 0s - 534us/step - loss: 1.9092\n",
      "Epoch 303/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9104\n",
      "Epoch 304/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9092\n",
      "Epoch 305/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9095\n",
      "Epoch 306/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9094\n",
      "Epoch 307/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9076\n",
      "Epoch 308/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9090\n",
      "Epoch 309/500\n",
      "105/105 - 0s - 534us/step - loss: 1.9094\n",
      "Epoch 310/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9075\n",
      "Epoch 311/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9083\n",
      "Epoch 312/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9087\n",
      "Epoch 313/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9099\n",
      "Epoch 314/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9088\n",
      "Epoch 315/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9094\n",
      "Epoch 316/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9081\n",
      "Epoch 317/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9093\n",
      "Epoch 318/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9099\n",
      "Epoch 319/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9076\n",
      "Epoch 320/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9112\n",
      "Epoch 321/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9100\n",
      "Epoch 322/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9089\n",
      "Epoch 323/500\n",
      "105/105 - 0s - 534us/step - loss: 1.9096\n",
      "Epoch 324/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9087\n",
      "Epoch 325/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9081\n",
      "Epoch 326/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9077\n",
      "Epoch 327/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9094\n",
      "Epoch 328/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9082\n",
      "Epoch 329/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9063\n",
      "Epoch 330/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9083\n",
      "Epoch 331/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9087\n",
      "Epoch 332/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9076\n",
      "Epoch 333/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9071\n",
      "Epoch 334/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9086\n",
      "Epoch 335/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9078\n",
      "Epoch 336/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9088\n",
      "Epoch 337/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9083\n",
      "Epoch 338/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9079\n",
      "Epoch 339/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9076\n",
      "Epoch 340/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9089\n",
      "Epoch 341/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9074\n",
      "Epoch 342/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9065\n",
      "Epoch 343/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9071\n",
      "Epoch 344/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9082\n",
      "Epoch 345/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9059\n",
      "Epoch 346/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9062\n",
      "Epoch 347/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9072\n",
      "Epoch 348/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9101\n",
      "Epoch 349/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9065\n",
      "Epoch 350/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9065\n",
      "Epoch 351/500\n",
      "105/105 - 0s - 610us/step - loss: 1.9054\n",
      "Epoch 352/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9070\n",
      "Epoch 353/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9110\n",
      "Epoch 354/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9055\n",
      "Epoch 355/500\n",
      "105/105 - 0s - 534us/step - loss: 1.9068\n",
      "Epoch 356/500\n",
      "105/105 - 0s - 581us/step - loss: 1.9053\n",
      "Epoch 357/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9069\n",
      "Epoch 358/500\n",
      "105/105 - 0s - 553us/step - loss: 1.9069\n",
      "Epoch 359/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9076\n",
      "Epoch 360/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9051\n",
      "Epoch 361/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9063\n",
      "Epoch 362/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9076\n",
      "Epoch 363/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9065\n",
      "Epoch 364/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9042\n",
      "Epoch 365/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9075\n",
      "Epoch 366/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9051\n",
      "Epoch 367/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9049\n",
      "Epoch 368/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9032\n",
      "Epoch 369/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9045\n",
      "Epoch 370/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9045\n",
      "Epoch 371/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9058\n",
      "Epoch 372/500\n",
      "105/105 - 0s - 571us/step - loss: 1.9046\n",
      "Epoch 373/500\n",
      "105/105 - 0s - 752us/step - loss: 1.9054\n",
      "Epoch 374/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9049\n",
      "Epoch 375/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9043\n",
      "Epoch 376/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9045\n",
      "Epoch 377/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9038\n",
      "Epoch 378/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9039\n",
      "Epoch 379/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9033\n",
      "Epoch 380/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9025\n",
      "Epoch 381/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9053\n",
      "Epoch 382/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9041\n",
      "Epoch 383/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9024\n",
      "Epoch 384/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9025\n",
      "Epoch 385/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9018\n",
      "Epoch 386/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9065\n",
      "Epoch 387/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9010\n",
      "Epoch 388/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9022\n",
      "Epoch 389/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9030\n",
      "Epoch 390/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9016\n",
      "Epoch 391/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9007\n",
      "Epoch 392/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9030\n",
      "Epoch 393/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9033\n",
      "Epoch 394/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9037\n",
      "Epoch 395/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9005\n",
      "Epoch 396/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9010\n",
      "Epoch 397/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9016\n",
      "Epoch 398/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9025\n",
      "Epoch 399/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9017\n",
      "Epoch 400/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9011\n",
      "Epoch 401/500\n",
      "105/105 - 0s - 524us/step - loss: 1.9025\n",
      "Epoch 402/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8994\n",
      "Epoch 403/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9042\n",
      "Epoch 404/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9002\n",
      "Epoch 405/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9026\n",
      "Epoch 406/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9005\n",
      "Epoch 407/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9009\n",
      "Epoch 408/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9013\n",
      "Epoch 409/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9004\n",
      "Epoch 410/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9006\n",
      "Epoch 411/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8995\n",
      "Epoch 412/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9004\n",
      "Epoch 413/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9005\n",
      "Epoch 414/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8993\n",
      "Epoch 415/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8997\n",
      "Epoch 416/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9009\n",
      "Epoch 417/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8987\n",
      "Epoch 418/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8992\n",
      "Epoch 419/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9023\n",
      "Epoch 420/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8984\n",
      "Epoch 421/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8981\n",
      "Epoch 422/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8990\n",
      "Epoch 423/500\n",
      "105/105 - 0s - 562us/step - loss: 1.9023\n",
      "Epoch 424/500\n",
      "105/105 - 0s - 543us/step - loss: 1.9027\n",
      "Epoch 425/500\n",
      "105/105 - 0s - 552us/step - loss: 1.9005\n",
      "Epoch 426/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8995\n",
      "Epoch 427/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8972\n",
      "Epoch 428/500\n",
      "105/105 - 0s - 542us/step - loss: 1.8977\n",
      "Epoch 429/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8984\n",
      "Epoch 430/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8976\n",
      "Epoch 431/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8982\n",
      "Epoch 432/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8972\n",
      "Epoch 433/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8975\n",
      "Epoch 434/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8975\n",
      "Epoch 435/500\n",
      "105/105 - 0s - 562us/step - loss: 1.8991\n",
      "Epoch 436/500\n",
      "105/105 - 0s - 524us/step - loss: 1.8978\n",
      "Epoch 437/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8979\n",
      "Epoch 438/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8984\n",
      "Epoch 439/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8954\n",
      "Epoch 440/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8971\n",
      "Epoch 441/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8982\n",
      "Epoch 442/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8972\n",
      "Epoch 443/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8964\n",
      "Epoch 444/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8975\n",
      "Epoch 445/500\n",
      "105/105 - 0s - 562us/step - loss: 1.8967\n",
      "Epoch 446/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8971\n",
      "Epoch 447/500\n",
      "105/105 - 0s - 533us/step - loss: 1.9000\n",
      "Epoch 448/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8982\n",
      "Epoch 449/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8973\n",
      "Epoch 450/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8968\n",
      "Epoch 451/500\n",
      "105/105 - 0s - 590us/step - loss: 1.8958\n",
      "Epoch 452/500\n",
      "105/105 - 0s - 619us/step - loss: 1.8977\n",
      "Epoch 453/500\n",
      "105/105 - 0s - 581us/step - loss: 1.8977\n",
      "Epoch 454/500\n",
      "105/105 - 0s - 553us/step - loss: 1.8962\n",
      "Epoch 455/500\n",
      "105/105 - 0s - 562us/step - loss: 1.8977\n",
      "Epoch 456/500\n",
      "105/105 - 0s - 553us/step - loss: 1.8954\n",
      "Epoch 457/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8973\n",
      "Epoch 458/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8976\n",
      "Epoch 459/500\n",
      "105/105 - 0s - 562us/step - loss: 1.8963\n",
      "Epoch 460/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8951\n",
      "Epoch 461/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8949\n",
      "Epoch 462/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8955\n",
      "Epoch 463/500\n",
      "105/105 - 0s - 524us/step - loss: 1.8977\n",
      "Epoch 464/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8941\n",
      "Epoch 465/500\n",
      "105/105 - 0s - 600us/step - loss: 1.8958\n",
      "Epoch 466/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8958\n",
      "Epoch 467/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8948\n",
      "Epoch 468/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8953\n",
      "Epoch 469/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8958\n",
      "Epoch 470/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8938\n",
      "Epoch 471/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8935\n",
      "Epoch 472/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8945\n",
      "Epoch 473/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8927\n",
      "Epoch 474/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8957\n",
      "Epoch 475/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8946\n",
      "Epoch 476/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8943\n",
      "Epoch 477/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8942\n",
      "Epoch 478/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8950\n",
      "Epoch 479/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8946\n",
      "Epoch 480/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8950\n",
      "Epoch 481/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8935\n",
      "Epoch 482/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8929\n",
      "Epoch 483/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8944\n",
      "Epoch 484/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8934\n",
      "Epoch 485/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8932\n",
      "Epoch 486/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8949\n",
      "Epoch 487/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8955\n",
      "Epoch 488/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8930\n",
      "Epoch 489/500\n",
      "105/105 - 0s - 562us/step - loss: 1.8927\n",
      "Epoch 490/500\n",
      "105/105 - 0s - 571us/step - loss: 1.8952\n",
      "Epoch 491/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8955\n",
      "Epoch 492/500\n",
      "105/105 - 0s - 542us/step - loss: 1.8912\n",
      "Epoch 493/500\n",
      "105/105 - 0s - 572us/step - loss: 1.8952\n",
      "Epoch 494/500\n",
      "105/105 - 0s - 552us/step - loss: 1.8927\n",
      "Epoch 495/500\n",
      "105/105 - 0s - 581us/step - loss: 1.8924\n",
      "Epoch 496/500\n",
      "105/105 - 0s - 562us/step - loss: 1.8942\n",
      "Epoch 497/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8927\n",
      "Epoch 498/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8936\n",
      "Epoch 499/500\n",
      "105/105 - 0s - 533us/step - loss: 1.8941\n",
      "Epoch 500/500\n",
      "105/105 - 0s - 543us/step - loss: 1.8953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x142c3c155b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Accuracy: 0.293\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tying this all together, the complete example of an MLP neural network for the abalone dataset framed as a classification problem is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\ML-classification-regression\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - 6ms/step - loss: 3.3343\n",
      "Epoch 2/150\n",
      "88/88 - 0s - 591us/step - loss: 2.8147\n",
      "Epoch 3/150\n",
      "88/88 - 0s - 580us/step - loss: 2.4909\n",
      "Epoch 4/150\n",
      "88/88 - 0s - 568us/step - loss: 2.3897\n",
      "Epoch 5/150\n",
      "88/88 - 0s - 568us/step - loss: 2.3136\n",
      "Epoch 6/150\n",
      "88/88 - 0s - 579us/step - loss: 2.2665\n",
      "Epoch 7/150\n",
      "88/88 - 0s - 568us/step - loss: 2.2355\n",
      "Epoch 8/150\n",
      "88/88 - 0s - 568us/step - loss: 2.2172\n",
      "Epoch 9/150\n",
      "88/88 - 0s - 602us/step - loss: 2.2022\n",
      "Epoch 10/150\n",
      "88/88 - 0s - 591us/step - loss: 2.1884\n",
      "Epoch 11/150\n",
      "88/88 - 0s - 580us/step - loss: 2.1771\n",
      "Epoch 12/150\n",
      "88/88 - 0s - 568us/step - loss: 2.1666\n",
      "Epoch 13/150\n",
      "88/88 - 0s - 580us/step - loss: 2.1565\n",
      "Epoch 14/150\n",
      "88/88 - 0s - 579us/step - loss: 2.1472\n",
      "Epoch 15/150\n",
      "88/88 - 0s - 580us/step - loss: 2.1367\n",
      "Epoch 16/150\n",
      "88/88 - 0s - 591us/step - loss: 2.1281\n",
      "Epoch 17/150\n",
      "88/88 - 0s - 591us/step - loss: 2.1204\n",
      "Epoch 18/150\n",
      "88/88 - 0s - 568us/step - loss: 2.1127\n",
      "Epoch 19/150\n",
      "88/88 - 0s - 568us/step - loss: 2.1053\n",
      "Epoch 20/150\n",
      "88/88 - 0s - 568us/step - loss: 2.0962\n",
      "Epoch 21/150\n",
      "88/88 - 0s - 580us/step - loss: 2.0879\n",
      "Epoch 22/150\n",
      "88/88 - 0s - 568us/step - loss: 2.0822\n",
      "Epoch 23/150\n",
      "88/88 - 0s - 591us/step - loss: 2.0754\n",
      "Epoch 24/150\n",
      "88/88 - 0s - 579us/step - loss: 2.0684\n",
      "Epoch 25/150\n",
      "88/88 - 0s - 568us/step - loss: 2.0614\n",
      "Epoch 26/150\n",
      "88/88 - 0s - 568us/step - loss: 2.0555\n",
      "Epoch 27/150\n",
      "88/88 - 0s - 568us/step - loss: 2.0487\n",
      "Epoch 28/150\n",
      "88/88 - 0s - 568us/step - loss: 2.0435\n",
      "Epoch 29/150\n",
      "88/88 - 0s - 602us/step - loss: 2.0383\n",
      "Epoch 30/150\n",
      "88/88 - 0s - 568us/step - loss: 2.0342\n",
      "Epoch 31/150\n",
      "88/88 - 0s - 557us/step - loss: 2.0273\n",
      "Epoch 32/150\n",
      "88/88 - 0s - 568us/step - loss: 2.0234\n",
      "Epoch 33/150\n",
      "88/88 - 0s - 602us/step - loss: 2.0191\n",
      "Epoch 34/150\n",
      "88/88 - 0s - 580us/step - loss: 2.0161\n",
      "Epoch 35/150\n",
      "88/88 - 0s - 568us/step - loss: 2.0087\n",
      "Epoch 36/150\n",
      "88/88 - 0s - 580us/step - loss: 2.0085\n",
      "Epoch 37/150\n",
      "88/88 - 0s - 568us/step - loss: 2.0026\n",
      "Epoch 38/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9997\n",
      "Epoch 39/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9976\n",
      "Epoch 40/150\n",
      "88/88 - 0s - 625us/step - loss: 1.9922\n",
      "Epoch 41/150\n",
      "88/88 - 0s - 636us/step - loss: 1.9916\n",
      "Epoch 42/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9871\n",
      "Epoch 43/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9846\n",
      "Epoch 44/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9824\n",
      "Epoch 45/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9815\n",
      "Epoch 46/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9786\n",
      "Epoch 47/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9761\n",
      "Epoch 48/150\n",
      "88/88 - 0s - 807us/step - loss: 1.9746\n",
      "Epoch 49/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9720\n",
      "Epoch 50/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9702\n",
      "Epoch 51/150\n",
      "88/88 - 0s - 613us/step - loss: 1.9688\n",
      "Epoch 52/150\n",
      "88/88 - 0s - 602us/step - loss: 1.9670\n",
      "Epoch 53/150\n",
      "88/88 - 0s - 602us/step - loss: 1.9657\n",
      "Epoch 54/150\n",
      "88/88 - 0s - 614us/step - loss: 1.9655\n",
      "Epoch 55/150\n",
      "88/88 - 0s - 625us/step - loss: 1.9641\n",
      "Epoch 56/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9626\n",
      "Epoch 57/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9617\n",
      "Epoch 58/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9598\n",
      "Epoch 59/150\n",
      "88/88 - 0s - 579us/step - loss: 1.9584\n",
      "Epoch 60/150\n",
      "88/88 - 0s - 579us/step - loss: 1.9583\n",
      "Epoch 61/150\n",
      "88/88 - 0s - 614us/step - loss: 1.9558\n",
      "Epoch 62/150\n",
      "88/88 - 0s - 625us/step - loss: 1.9538\n",
      "Epoch 63/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9559\n",
      "Epoch 64/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9552\n",
      "Epoch 65/150\n",
      "88/88 - 0s - 579us/step - loss: 1.9529\n",
      "Epoch 66/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9518\n",
      "Epoch 67/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9531\n",
      "Epoch 68/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9516\n",
      "Epoch 69/150\n",
      "88/88 - 0s - 579us/step - loss: 1.9497\n",
      "Epoch 70/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9503\n",
      "Epoch 71/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9485\n",
      "Epoch 72/150\n",
      "88/88 - 0s - 625us/step - loss: 1.9483\n",
      "Epoch 73/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9482\n",
      "Epoch 74/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9497\n",
      "Epoch 75/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9488\n",
      "Epoch 76/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9491\n",
      "Epoch 77/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9491\n",
      "Epoch 78/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9461\n",
      "Epoch 79/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9458\n",
      "Epoch 80/150\n",
      "88/88 - 0s - 579us/step - loss: 1.9466\n",
      "Epoch 81/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9463\n",
      "Epoch 82/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9448\n",
      "Epoch 83/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9442\n",
      "Epoch 84/150\n",
      "88/88 - 0s - 579us/step - loss: 1.9432\n",
      "Epoch 85/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9436\n",
      "Epoch 86/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9461\n",
      "Epoch 87/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9423\n",
      "Epoch 88/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9428\n",
      "Epoch 89/150\n",
      "88/88 - 0s - 602us/step - loss: 1.9423\n",
      "Epoch 90/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9415\n",
      "Epoch 91/150\n",
      "88/88 - 0s - 625us/step - loss: 1.9404\n",
      "Epoch 92/150\n",
      "88/88 - 0s - 625us/step - loss: 1.9427\n",
      "Epoch 93/150\n",
      "88/88 - 0s - 614us/step - loss: 1.9411\n",
      "Epoch 94/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9393\n",
      "Epoch 95/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9396\n",
      "Epoch 96/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9400\n",
      "Epoch 97/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9398\n",
      "Epoch 98/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9399\n",
      "Epoch 99/150\n",
      "88/88 - 0s - 625us/step - loss: 1.9391\n",
      "Epoch 100/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9404\n",
      "Epoch 101/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9398\n",
      "Epoch 102/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9380\n",
      "Epoch 103/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9380\n",
      "Epoch 104/150\n",
      "88/88 - 0s - 579us/step - loss: 1.9396\n",
      "Epoch 105/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9375\n",
      "Epoch 106/150\n",
      "88/88 - 0s - 625us/step - loss: 1.9387\n",
      "Epoch 107/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9383\n",
      "Epoch 108/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9362\n",
      "Epoch 109/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9367\n",
      "Epoch 110/150\n",
      "88/88 - 0s - 579us/step - loss: 1.9368\n",
      "Epoch 111/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9362\n",
      "Epoch 112/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9358\n",
      "Epoch 113/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9384\n",
      "Epoch 114/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9373\n",
      "Epoch 115/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9352\n",
      "Epoch 116/150\n",
      "88/88 - 0s - 579us/step - loss: 1.9371\n",
      "Epoch 117/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9354\n",
      "Epoch 118/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9360\n",
      "Epoch 119/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9322\n",
      "Epoch 120/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9359\n",
      "Epoch 121/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9363\n",
      "Epoch 122/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9344\n",
      "Epoch 123/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9334\n",
      "Epoch 124/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9355\n",
      "Epoch 125/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9322\n",
      "Epoch 126/150\n",
      "88/88 - 0s - 602us/step - loss: 1.9319\n",
      "Epoch 127/150\n",
      "88/88 - 0s - 602us/step - loss: 1.9323\n",
      "Epoch 128/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9303\n",
      "Epoch 129/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9313\n",
      "Epoch 130/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9300\n",
      "Epoch 131/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9297\n",
      "Epoch 132/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9286\n",
      "Epoch 133/150\n",
      "88/88 - 0s - 579us/step - loss: 1.9263\n",
      "Epoch 134/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9281\n",
      "Epoch 135/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9291\n",
      "Epoch 136/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9273\n",
      "Epoch 137/150\n",
      "88/88 - 0s - 580us/step - loss: 1.9282\n",
      "Epoch 138/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9267\n",
      "Epoch 139/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9264\n",
      "Epoch 140/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9264\n",
      "Epoch 141/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9265\n",
      "Epoch 142/150\n",
      "88/88 - 0s - 557us/step - loss: 1.9261\n",
      "Epoch 143/150\n",
      "88/88 - 0s - 614us/step - loss: 1.9263\n",
      "Epoch 144/150\n",
      "88/88 - 0s - 602us/step - loss: 1.9247\n",
      "Epoch 145/150\n",
      "88/88 - 0s - 591us/step - loss: 1.9242\n",
      "Epoch 146/150\n",
      "88/88 - 0s - 773us/step - loss: 1.9241\n",
      "Epoch 147/150\n",
      "88/88 - 0s - 625us/step - loss: 1.9244\n",
      "Epoch 148/150\n",
      "88/88 - 0s - 602us/step - loss: 1.9225\n",
      "Epoch 149/150\n",
      "88/88 - 0s - 602us/step - loss: 1.9263\n",
      "Epoch 150/150\n",
      "88/88 - 0s - 568us/step - loss: 1.9242\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step\n",
      "Accuracy: 0.272\n"
     ]
    }
   ],
   "source": [
    "# classification mlp model for the abalone dataset\n",
    "from numpy import unique\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (y) variables\n",
    "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
    "X, y = X.astype('float'), y.astype('float')\n",
    "n_features = X.shape[1]\n",
    "# encode strings to integer\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "n_class = len(unique(y))\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(n_class, activation='softmax'))\n",
    "# compile the keras model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined Regression and Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp for combined regression and classification predictions on the abalone dataset\n",
    "from numpy import unique\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (y) variables\n",
    "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
    "X, y = X.astype('float'), y.astype('float')\n",
    "n_features = X.shape[1]\n",
    "# encode strings to integer\n",
    "y_class = LabelEncoder().fit_transform(y)\n",
    "n_class = len(unique(y_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test, y_train_class, y_test_class = train_test_split(X, y, y_class, test_size=0.33, random_state=1)\n",
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "visible = Input(shape=(n_features,))\n",
    "hidden1 = Dense(20, activation='relu', kernel_initializer='he_normal')(visible)\n",
    "hidden2 = Dense(10, activation='relu', kernel_initializer='he_normal')(hidden1)\n",
    "# regression output\n",
    "out_reg = Dense(1, activation='linear')(hidden2)\n",
    "# classification output\n",
    "out_clas = Dense(n_class, activation='softmax')(hidden2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Model(inputs=visible, outputs=[out_reg, out_clas])\n",
    "# compile the keras model\n",
    "model.compile(loss=['mse','sparse_categorical_crossentropy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span> │ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">308</span> │ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │        \u001b[38;5;34m160\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │        \u001b[38;5;34m210\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m11\u001b[0m │ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)        │        \u001b[38;5;34m308\u001b[0m │ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">689</span> (2.69 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m689\u001b[0m (2.69 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">689</span> (2.69 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m689\u001b[0m (2.69 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from tensorflow.keras import utils\n",
    "from tensorflow.keras.utils import plot_model\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "88/88 - 1s - 6ms/step - loss: 110.4246\n",
      "Epoch 2/150\n",
      "88/88 - 0s - 625us/step - loss: 95.5037\n",
      "Epoch 3/150\n",
      "88/88 - 0s - 614us/step - loss: 63.8956\n",
      "Epoch 4/150\n",
      "88/88 - 0s - 636us/step - loss: 22.8469\n",
      "Epoch 5/150\n",
      "88/88 - 0s - 636us/step - loss: 10.7343\n",
      "Epoch 6/150\n",
      "88/88 - 0s - 602us/step - loss: 10.2279\n",
      "Epoch 7/150\n",
      "88/88 - 0s - 614us/step - loss: 9.9814\n",
      "Epoch 8/150\n",
      "88/88 - 0s - 614us/step - loss: 9.7724\n",
      "Epoch 9/150\n",
      "88/88 - 0s - 625us/step - loss: 9.6113\n",
      "Epoch 10/150\n",
      "88/88 - 0s - 625us/step - loss: 9.4948\n",
      "Epoch 11/150\n",
      "88/88 - 0s - 636us/step - loss: 9.4090\n",
      "Epoch 12/150\n",
      "88/88 - 0s - 648us/step - loss: 9.3340\n",
      "Epoch 13/150\n",
      "88/88 - 0s - 636us/step - loss: 9.2598\n",
      "Epoch 14/150\n",
      "88/88 - 0s - 614us/step - loss: 9.1976\n",
      "Epoch 15/150\n",
      "88/88 - 0s - 625us/step - loss: 9.1172\n",
      "Epoch 16/150\n",
      "88/88 - 0s - 625us/step - loss: 9.0816\n",
      "Epoch 17/150\n",
      "88/88 - 0s - 614us/step - loss: 8.9960\n",
      "Epoch 18/150\n",
      "88/88 - 0s - 614us/step - loss: 8.9365\n",
      "Epoch 19/150\n",
      "88/88 - 0s - 625us/step - loss: 8.8650\n",
      "Epoch 20/150\n",
      "88/88 - 0s - 625us/step - loss: 8.8199\n",
      "Epoch 21/150\n",
      "88/88 - 0s - 614us/step - loss: 8.7381\n",
      "Epoch 22/150\n",
      "88/88 - 0s - 625us/step - loss: 8.7040\n",
      "Epoch 23/150\n",
      "88/88 - 0s - 636us/step - loss: 8.6176\n",
      "Epoch 24/150\n",
      "88/88 - 0s - 636us/step - loss: 8.5582\n",
      "Epoch 25/150\n",
      "88/88 - 0s - 636us/step - loss: 8.4932\n",
      "Epoch 26/150\n",
      "88/88 - 0s - 625us/step - loss: 8.4356\n",
      "Epoch 27/150\n",
      "88/88 - 0s - 636us/step - loss: 8.3635\n",
      "Epoch 28/150\n",
      "88/88 - 0s - 625us/step - loss: 8.2975\n",
      "Epoch 29/150\n",
      "88/88 - 0s - 648us/step - loss: 8.2317\n",
      "Epoch 30/150\n",
      "88/88 - 0s - 648us/step - loss: 8.1614\n",
      "Epoch 31/150\n",
      "88/88 - 0s - 614us/step - loss: 8.0946\n",
      "Epoch 32/150\n",
      "88/88 - 0s - 636us/step - loss: 8.0245\n",
      "Epoch 33/150\n",
      "88/88 - 0s - 636us/step - loss: 7.9659\n",
      "Epoch 34/150\n",
      "88/88 - 0s - 625us/step - loss: 7.8487\n",
      "Epoch 35/150\n",
      "88/88 - 0s - 636us/step - loss: 7.7746\n",
      "Epoch 36/150\n",
      "88/88 - 0s - 614us/step - loss: 7.7163\n",
      "Epoch 37/150\n",
      "88/88 - 0s - 614us/step - loss: 7.6425\n",
      "Epoch 38/150\n",
      "88/88 - 0s - 625us/step - loss: 7.5568\n",
      "Epoch 39/150\n",
      "88/88 - 0s - 614us/step - loss: 7.5088\n",
      "Epoch 40/150\n",
      "88/88 - 0s - 614us/step - loss: 7.4655\n",
      "Epoch 41/150\n",
      "88/88 - 0s - 614us/step - loss: 7.4007\n",
      "Epoch 42/150\n",
      "88/88 - 0s - 614us/step - loss: 7.3323\n",
      "Epoch 43/150\n",
      "88/88 - 0s - 625us/step - loss: 7.3125\n",
      "Epoch 44/150\n",
      "88/88 - 0s - 625us/step - loss: 7.2490\n",
      "Epoch 45/150\n",
      "88/88 - 0s - 625us/step - loss: 7.2057\n",
      "Epoch 46/150\n",
      "88/88 - 0s - 636us/step - loss: 7.1831\n",
      "Epoch 47/150\n",
      "88/88 - 0s - 648us/step - loss: 7.1610\n",
      "Epoch 48/150\n",
      "88/88 - 0s - 625us/step - loss: 7.1423\n",
      "Epoch 49/150\n",
      "88/88 - 0s - 625us/step - loss: 7.1039\n",
      "Epoch 50/150\n",
      "88/88 - 0s - 614us/step - loss: 7.0908\n",
      "Epoch 51/150\n",
      "88/88 - 0s - 625us/step - loss: 7.0749\n",
      "Epoch 52/150\n",
      "88/88 - 0s - 625us/step - loss: 7.0780\n",
      "Epoch 53/150\n",
      "88/88 - 0s - 625us/step - loss: 7.0339\n",
      "Epoch 54/150\n",
      "88/88 - 0s - 648us/step - loss: 7.0339\n",
      "Epoch 55/150\n",
      "88/88 - 0s - 614us/step - loss: 6.9953\n",
      "Epoch 56/150\n",
      "88/88 - 0s - 614us/step - loss: 7.0108\n",
      "Epoch 57/150\n",
      "88/88 - 0s - 636us/step - loss: 6.9829\n",
      "Epoch 58/150\n",
      "88/88 - 0s - 636us/step - loss: 6.9668\n",
      "Epoch 59/150\n",
      "88/88 - 0s - 648us/step - loss: 6.9982\n",
      "Epoch 60/150\n",
      "88/88 - 0s - 625us/step - loss: 6.9528\n",
      "Epoch 61/150\n",
      "88/88 - 0s - 625us/step - loss: 6.9794\n",
      "Epoch 62/150\n",
      "88/88 - 0s - 614us/step - loss: 6.9521\n",
      "Epoch 63/150\n",
      "88/88 - 0s - 614us/step - loss: 6.9419\n",
      "Epoch 64/150\n",
      "88/88 - 0s - 613us/step - loss: 6.9268\n",
      "Epoch 65/150\n",
      "88/88 - 0s - 636us/step - loss: 6.9303\n",
      "Epoch 66/150\n",
      "88/88 - 0s - 614us/step - loss: 6.9506\n",
      "Epoch 67/150\n",
      "88/88 - 0s - 614us/step - loss: 6.9066\n",
      "Epoch 68/150\n",
      "88/88 - 0s - 614us/step - loss: 6.8818\n",
      "Epoch 69/150\n",
      "88/88 - 0s - 625us/step - loss: 6.8729\n",
      "Epoch 70/150\n",
      "88/88 - 0s - 614us/step - loss: 6.8777\n",
      "Epoch 71/150\n",
      "88/88 - 0s - 602us/step - loss: 6.8877\n",
      "Epoch 72/150\n",
      "88/88 - 0s - 602us/step - loss: 6.8566\n",
      "Epoch 73/150\n",
      "88/88 - 0s - 614us/step - loss: 6.8654\n",
      "Epoch 74/150\n",
      "88/88 - 0s - 614us/step - loss: 6.8667\n",
      "Epoch 75/150\n",
      "88/88 - 0s - 602us/step - loss: 6.8391\n",
      "Epoch 76/150\n",
      "88/88 - 0s - 614us/step - loss: 6.8697\n",
      "Epoch 77/150\n",
      "88/88 - 0s - 625us/step - loss: 6.8523\n",
      "Epoch 78/150\n",
      "88/88 - 0s - 625us/step - loss: 6.8184\n",
      "Epoch 79/150\n",
      "88/88 - 0s - 625us/step - loss: 6.8369\n",
      "Epoch 80/150\n",
      "88/88 - 0s - 614us/step - loss: 6.8307\n",
      "Epoch 81/150\n",
      "88/88 - 0s - 625us/step - loss: 6.8385\n",
      "Epoch 82/150\n",
      "88/88 - 0s - 614us/step - loss: 6.8113\n",
      "Epoch 83/150\n",
      "88/88 - 0s - 625us/step - loss: 6.8115\n",
      "Epoch 84/150\n",
      "88/88 - 0s - 614us/step - loss: 6.8266\n",
      "Epoch 85/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7934\n",
      "Epoch 86/150\n",
      "88/88 - 0s - 602us/step - loss: 6.8134\n",
      "Epoch 87/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7859\n",
      "Epoch 88/150\n",
      "88/88 - 0s - 614us/step - loss: 6.8209\n",
      "Epoch 89/150\n",
      "88/88 - 0s - 625us/step - loss: 6.8560\n",
      "Epoch 90/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7784\n",
      "Epoch 91/150\n",
      "88/88 - 0s - 636us/step - loss: 6.8023\n",
      "Epoch 92/150\n",
      "88/88 - 0s - 625us/step - loss: 6.8140\n",
      "Epoch 93/150\n",
      "88/88 - 0s - 625us/step - loss: 6.8456\n",
      "Epoch 94/150\n",
      "88/88 - 0s - 614us/step - loss: 6.8272\n",
      "Epoch 95/150\n",
      "88/88 - 0s - 625us/step - loss: 6.7741\n",
      "Epoch 96/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7637\n",
      "Epoch 97/150\n",
      "88/88 - 0s - 602us/step - loss: 6.7640\n",
      "Epoch 98/150\n",
      "88/88 - 0s - 602us/step - loss: 6.7539\n",
      "Epoch 99/150\n",
      "88/88 - 0s - 636us/step - loss: 6.7560\n",
      "Epoch 100/150\n",
      "88/88 - 0s - 875us/step - loss: 6.7657\n",
      "Epoch 101/150\n",
      "88/88 - 0s - 659us/step - loss: 6.7468\n",
      "Epoch 102/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7324\n",
      "Epoch 103/150\n",
      "88/88 - 0s - 625us/step - loss: 6.7559\n",
      "Epoch 104/150\n",
      "88/88 - 0s - 637us/step - loss: 6.7508\n",
      "Epoch 105/150\n",
      "88/88 - 0s - 670us/step - loss: 6.7368\n",
      "Epoch 106/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7122\n",
      "Epoch 107/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7508\n",
      "Epoch 108/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7204\n",
      "Epoch 109/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7233\n",
      "Epoch 110/150\n",
      "88/88 - 0s - 602us/step - loss: 6.7020\n",
      "Epoch 111/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7238\n",
      "Epoch 112/150\n",
      "88/88 - 0s - 636us/step - loss: 6.7406\n",
      "Epoch 113/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7236\n",
      "Epoch 114/150\n",
      "88/88 - 0s - 625us/step - loss: 6.7216\n",
      "Epoch 115/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7045\n",
      "Epoch 116/150\n",
      "88/88 - 0s - 625us/step - loss: 6.7068\n",
      "Epoch 117/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7002\n",
      "Epoch 118/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7060\n",
      "Epoch 119/150\n",
      "88/88 - 0s - 614us/step - loss: 6.6815\n",
      "Epoch 120/150\n",
      "88/88 - 0s - 614us/step - loss: 6.6923\n",
      "Epoch 121/150\n",
      "88/88 - 0s - 614us/step - loss: 6.7032\n",
      "Epoch 122/150\n",
      "88/88 - 0s - 614us/step - loss: 6.6882\n",
      "Epoch 123/150\n",
      "88/88 - 0s - 648us/step - loss: 6.6733\n",
      "Epoch 124/150\n",
      "88/88 - 0s - 636us/step - loss: 6.6904\n",
      "Epoch 125/150\n",
      "88/88 - 0s - 625us/step - loss: 6.6580\n",
      "Epoch 126/150\n",
      "88/88 - 0s - 602us/step - loss: 6.6853\n",
      "Epoch 127/150\n",
      "88/88 - 0s - 625us/step - loss: 6.6822\n",
      "Epoch 128/150\n",
      "88/88 - 0s - 648us/step - loss: 6.6575\n",
      "Epoch 129/150\n",
      "88/88 - 0s - 614us/step - loss: 6.6640\n",
      "Epoch 130/150\n",
      "88/88 - 0s - 602us/step - loss: 6.7221\n",
      "Epoch 131/150\n",
      "88/88 - 0s - 636us/step - loss: 6.6697\n",
      "Epoch 132/150\n",
      "88/88 - 0s - 625us/step - loss: 6.6671\n",
      "Epoch 133/150\n",
      "88/88 - 0s - 636us/step - loss: 6.6854\n",
      "Epoch 134/150\n",
      "88/88 - 0s - 625us/step - loss: 6.6423\n",
      "Epoch 135/150\n",
      "88/88 - 0s - 625us/step - loss: 6.6491\n",
      "Epoch 136/150\n",
      "88/88 - 0s - 614us/step - loss: 6.6571\n",
      "Epoch 137/150\n",
      "88/88 - 0s - 625us/step - loss: 6.6503\n",
      "Epoch 138/150\n",
      "88/88 - 0s - 625us/step - loss: 6.6574\n",
      "Epoch 139/150\n",
      "88/88 - 0s - 625us/step - loss: 6.6321\n",
      "Epoch 140/150\n",
      "88/88 - 0s - 625us/step - loss: 6.6409\n",
      "Epoch 141/150\n",
      "88/88 - 0s - 636us/step - loss: 6.6293\n",
      "Epoch 142/150\n",
      "88/88 - 0s - 625us/step - loss: 6.6316\n",
      "Epoch 143/150\n",
      "88/88 - 0s - 614us/step - loss: 6.6308\n",
      "Epoch 144/150\n",
      "88/88 - 0s - 614us/step - loss: 6.6071\n",
      "Epoch 145/150\n",
      "88/88 - 0s - 625us/step - loss: 6.6108\n",
      "Epoch 146/150\n",
      "88/88 - 0s - 625us/step - loss: 6.6087\n",
      "Epoch 147/150\n",
      "88/88 - 0s - 625us/step - loss: 6.6054\n",
      "Epoch 148/150\n",
      "88/88 - 0s - 602us/step - loss: 6.6029\n",
      "Epoch 149/150\n",
      "88/88 - 0s - 614us/step - loss: 6.6235\n",
      "Epoch 150/150\n",
      "88/88 - 0s - 602us/step - loss: 6.6378\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MAE: 1.553\n",
      "Accuracy: 0.268\n"
     ]
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, [y_train,y_train_class], epochs=150, batch_size=32, verbose=2)\n",
    "# make predictions on test set\n",
    "yhat1, yhat2 = model.predict(X_test)\n",
    "# calculate error for regression model\n",
    "error = mean_absolute_error(y_test, yhat1)\n",
    "print('MAE: %.3f' % error)\n",
    "# evaluate accuracy for classification model\n",
    "yhat2 = argmax(yhat2, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test_class, yhat2)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
